// Code generated by command: go run asm.go -out sip13_amd64.s. DO NOT EDIT.

// +build !noasm

#include "textflag.h"

// func Sum64(k0 uint64, k1 uint64, p []byte) uint64
TEXT ·Sum64(SB), NOSPLIT, $0-48
	MOVQ k0+0(FP), AX
	MOVQ AX, DX
	MOVQ k1+8(FP), CX
	MOVQ CX, BX
	MOVQ $0x736f6d6570736575, BP
	XORQ BP, AX
	MOVQ $0x646f72616e646f6d, BP
	XORQ BP, CX
	MOVQ $0x6c7967656e657261, BP
	XORQ BP, DX
	MOVQ $0x7465646279746573, BP
	XORQ BP, BX
	MOVQ p_base+16(FP), BP
	MOVQ p_len+24(FP), SI
	MOVQ SI, DI
	SHLQ $0x38, DI
	CMPQ SI, $0x08
	JL   loop_end

loop_begin:
	MOVQ (BP), R8
	XORQ R8, BX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ R8, AX
	ADDQ $0x08, BP
	SUBQ $0x08, SI
	CMPQ SI, $0x08
	JGE  loop_begin

loop_end:
	CMPQ    SI, $0x00
	JE      sw0
	CMPQ    SI, $0x01
	JE      sw1
	CMPQ    SI, $0x02
	JE      sw2
	CMPQ    SI, $0x03
	JE      sw3
	CMPQ    SI, $0x04
	JE      sw4
	CMPQ    SI, $0x05
	JE      sw5
	CMPQ    SI, $0x06
	JE      sw6
	MOVBQZX 6(BP), SI
	SHLQ    $0x30, SI
	ORQ     SI, DI

sw6:
	MOVBQZX 5(BP), SI
	SHLQ    $0x28, SI
	ORQ     SI, DI

sw5:
	MOVBQZX 4(BP), SI
	SHLQ    $0x20, SI
	ORQ     SI, DI

sw4:
	MOVBQZX 3(BP), SI
	SHLQ    $0x18, SI
	ORQ     SI, DI

sw3:
	MOVBQZX 2(BP), SI
	SHLQ    $0x10, SI
	ORQ     SI, DI

sw2:
	MOVBQZX 1(BP), SI
	SHLQ    $0x08, SI
	ORQ     SI, DI

sw1:
	MOVBQZX (BP), SI
	SHLQ    $0x00, SI
	ORQ     SI, DI

sw0:
	XORQ DI, BX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ DI, AX
	XORQ $0xff, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ CX, AX
	XORQ BX, DX
	XORQ DX, AX
	MOVQ AX, ret+40(FP)
	RET

// func Sum64Str(k0 uint64, k1 uint64, p string) uint64
TEXT ·Sum64Str(SB), NOSPLIT, $0-40
	MOVQ k0+0(FP), AX
	MOVQ AX, DX
	MOVQ k1+8(FP), CX
	MOVQ CX, BX
	MOVQ $0x736f6d6570736575, BP
	XORQ BP, AX
	MOVQ $0x646f72616e646f6d, BP
	XORQ BP, CX
	MOVQ $0x6c7967656e657261, BP
	XORQ BP, DX
	MOVQ $0x7465646279746573, BP
	XORQ BP, BX
	MOVQ p_base+16(FP), BP
	MOVQ p_len+24(FP), SI
	MOVQ SI, DI
	SHLQ $0x38, DI
	CMPQ SI, $0x08
	JL   loop_end

loop_begin:
	MOVQ (BP), R8
	XORQ R8, BX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ R8, AX
	ADDQ $0x08, BP
	SUBQ $0x08, SI
	CMPQ SI, $0x08
	JGE  loop_begin

loop_end:
	CMPQ    SI, $0x00
	JE      sw0
	CMPQ    SI, $0x01
	JE      sw1
	CMPQ    SI, $0x02
	JE      sw2
	CMPQ    SI, $0x03
	JE      sw3
	CMPQ    SI, $0x04
	JE      sw4
	CMPQ    SI, $0x05
	JE      sw5
	CMPQ    SI, $0x06
	JE      sw6
	MOVBQZX 6(BP), SI
	SHLQ    $0x30, SI
	ORQ     SI, DI

sw6:
	MOVBQZX 5(BP), SI
	SHLQ    $0x28, SI
	ORQ     SI, DI

sw5:
	MOVBQZX 4(BP), SI
	SHLQ    $0x20, SI
	ORQ     SI, DI

sw4:
	MOVBQZX 3(BP), SI
	SHLQ    $0x18, SI
	ORQ     SI, DI

sw3:
	MOVBQZX 2(BP), SI
	SHLQ    $0x10, SI
	ORQ     SI, DI

sw2:
	MOVBQZX 1(BP), SI
	SHLQ    $0x08, SI
	ORQ     SI, DI

sw1:
	MOVBQZX (BP), SI
	SHLQ    $0x00, SI
	ORQ     SI, DI

sw0:
	XORQ DI, BX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ DI, AX
	XORQ $0xff, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	ADDQ CX, AX
	ADDQ BX, DX
	ROLQ $0x0d, CX
	ROLQ $0x10, BX
	XORQ AX, CX
	XORQ DX, BX
	ROLQ $0x20, AX
	ADDQ CX, DX
	ADDQ BX, AX
	ROLQ $0x11, CX
	ROLQ $0x15, BX
	XORQ DX, CX
	XORQ AX, BX
	ROLQ $0x20, DX
	XORQ CX, AX
	XORQ BX, DX
	XORQ DX, AX
	MOVQ AX, ret+32(FP)
	RET
